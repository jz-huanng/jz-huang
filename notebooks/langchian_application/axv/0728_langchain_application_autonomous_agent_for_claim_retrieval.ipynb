{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### setup"
      ],
      "metadata": {
        "id": "S09Sl1kGLn8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive') # 此處需要登入google帳號\n",
        "\n",
        "!git clone https://github.com/IKMLab/NCKU-AICUP2023-baseline\n",
        "%cd NCKU-AICUP2023-baseline\n",
        "!cp -r /content/gdrive/MyDrive/aicup-2023-plain/* ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPNnt2uEQKgV",
        "outputId": "58750c0b-c08b-4d6c-b78b-8221a8431ff6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Cloning into 'NCKU-AICUP2023-baseline'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 146 (delta 34), reused 30 (delta 22), pack-reused 101\u001b[K\n",
            "Receiving objects: 100% (146/146), 82.83 KiB | 1.88 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "/content/NCKU-AICUP2023-baseline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIUoNn0YM7SU",
        "outputId": "8d7fe2ae-ea0f-4589-d5b5-d4610b6f7ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install -q ipywidgets pandarallel pandas scikit-learn tqdm opencc wikipedia\n",
        "!pip install -q pandas wikipedia\n",
        "!pip -q install openai langchain #huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# 3rd party libs\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "wikipedia.set_lang(\"zh\")\n",
        "\n",
        "import langchain\n",
        "langchain.debug=False\n"
      ],
      "metadata": {
        "id": "Pibak_jWNEbr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### functions(wiki...)"
      ],
      "metadata": {
        "id": "tmsC7j7sLdPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf8\") as json_file:\n",
        "        json_list = list(json_file)\n",
        "    return [json.loads(json_str) for json_str in json_list]\n",
        "\n",
        "def wiki_database(path):\n",
        "\t\tdatabase=pd.concat(\n",
        "        [pd.DataFrame(load_json(file)) for file in Path(path).glob(\"*.jsonl\")]\n",
        "    )\n",
        "\t\treturn dict(zip(database[\"id\"].tolist(),database[\"text\"].tolist()))\n",
        "\n",
        "def get_wiki_text(wiki,key_word):\n",
        "    try:\n",
        "      text=wiki[key_word]\n",
        "    except KeyError:\n",
        "      text=\"KeyError\"\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "DWgEnUIBPmGW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WIKI_PATH=\"data/wiki-pages\"\n",
        "\n",
        "wiki=wiki_database(WIKI_PATH)"
      ],
      "metadata": {
        "id": "qVlFZ_eQqcl_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build chains"
      ],
      "metadata": {
        "id": "ubyVZcs1NpnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "#text-davinci-003\n",
        "MODEL_NAME=\"gpt-3.5-turbo\"\n",
        "llm = OpenAI(model_name=MODEL_NAME, temperature=0, max_tokens = 256)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DLt9OzNHH1",
        "outputId": "db2e8087-7ac4-431a-c1bd-c3b7470295e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:200: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:801: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyChain(LLMChain):\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm_and_prompt(cls,llm,prompt,verbose=False):\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ],
      "metadata": {
        "id": "wNZUsAejHC8n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvidenceRetrivalBot(LLMChain):\n",
        "    \"\"\"implenment agent concept via chain class\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm_and_prompt(cls,llm,prompt,verbose=False):\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ],
      "metadata": {
        "id": "R_LaL3bFOhtH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "doc_retrieval_template = \"\"\"\n",
        "你是一個看過維基百科內容的機器人，讓我們來預測以下提供的陳述句可能的出處。請從陳述句中選擇一個單字作為檢索的關鍵字。\n",
        "\n",
        "claim:{claim}\n",
        "\n",
        "按照以下格式返回預測的關鍵詞\n",
        "keyword:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"claim\"],\n",
        "    template=doc_retrieval_template,\n",
        ")\n",
        "\n",
        "doc_chain=MyChain.from_llm_and_prompt(llm=llm,prompt=prompt)\n"
      ],
      "metadata": {
        "id": "oaUakMO5Nv0c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_selection_template = \"\"\"\n",
        "陳述句:{claim}\n",
        "\n",
        "已檢索的證據句:\"{evidence}\"\n",
        "\n",
        "目前的文章為\"{keyword}\":{text}\n",
        "\n",
        "上述的陳述句有可能是真實的也有可能虛假的，而當前檢索的文章為可靠的資訊來源。讓我們來完成事實查核任務，依照以下步驟先進行思考:\n",
        "\n",
        "思考:是否可以文章中檢索出與陳述句相關的句子作為證據句來\"支持或反駁\"陳述句?如果證據句無法說明陳述句的真偽，根據缺失的信息從句子當中找出關鍵字作為下一步需要檢索的頁面名稱。\n",
        "\n",
        "完成以上步驟並提供思考過程。\n",
        "\n",
        "根據思考的內容整理成\"純文字\"，依據對應指示填入答案。以下是內容整理的樣式以及相對應的指示:\n",
        "\n",
        "evidence:輸出從文章中檢索的句子並且不要使用: 數字 以及 · 以及 \\n 來區分每一句話。\n",
        "label:如果下一步檢索的文章名稱與當前文章相同，表示無法再找出缺失的信息，輸出\"Not Enough Info\"，否則無論是否需要再進行檢索文章，均為\"None\"\n",
        "doc:根據找出得關鍵字決定下一步需要檢索的文章名稱。如果不再進行檢索其他文章則為\"None\"\n",
        "state:如果需要繼續檢索其他文章則狀態為\"continue\"，如果已經找齊所有證據句則狀態為\"finish\"\n",
        "\"\"\"\n",
        "\n",
        "agent_planning = PromptTemplate(\n",
        "    input_variables=[\"claim\",\"evidence\",\"keyword\",\"text\"],\n",
        "    template=evidence_selection_template,\n",
        ")\n",
        "\n",
        "agent=EvidenceRetrivalBot.from_llm_and_prompt(llm=llm, prompt=agent_planning,verbose=True)\n"
      ],
      "metadata": {
        "id": "MPzxSj83Oumw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim_retrieval_template = \"\"\"\n",
        "根據| |前後的雙引號\"\"包圍的一組證據句以及\"\"包圍的陳述句，按照以下步驟以及要求，選擇使用 支持/反駁/資訊不足 來完成三分類任務\n",
        "\n",
        "一、從陳述句和證據句當中抽取出可以互相對應對應的單詞。\n",
        "\n",
        "二、接著檢出每一個對應的單詞，如果含義無法成功匹配，則視為關鍵詞。\n",
        "\n",
        "三、檢查證據句的關鍵詞是否有蘊含陳述句當中的關鍵詞，如果關鍵詞的含義不等同則視為反駁。不等同的狀況請參考以下但不限於以下範例：\"含義明顯衝突、無法經由推論得出結論、蘊含關係不明確。\"參考以上範例根據提供的句子做出合理的判斷。\n",
        "\n",
        "\"{evidence}\"| |\"{claim}\"\n",
        "\n",
        "根據上下文| |應填入 支持/反駁/資訊不足 ？\n",
        "\"\"\"\n",
        "\n",
        "claim_retrieval = PromptTemplate(\n",
        "    input_variables=[\"claim\",\"evidence\"],\n",
        "    template=claim_retrieval_template,\n",
        ")\n",
        "\n",
        "claimRetrieval=MyChain.from_llm_and_prompt(llm=llm, prompt=claim_retrieval,verbose=True)\n"
      ],
      "metadata": {
        "id": "mqS9JSqiQvme"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main"
      ],
      "metadata": {
        "id": "Ke342oxSfiOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "font ref:https://blog.csdn.net/c_lanxiaofang/article/details/126107796"
      ],
      "metadata": {
        "id": "9YEn1L-9Kavx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def keywordParser(keywords):\n",
        "    #\\s | Matches whitespace characters, which include the \\t, \\n, \\r, and space characters.\n",
        "    PATTERN=re.compile(r\"(K|k)eywords?\\s*:?\\s*\\'*\\\"*\")\n",
        "    keyword=re.split(PATTERN,keywords)[-1]\n",
        "    return keyword\n",
        "\n",
        "def getAction(response):\n",
        "    PARSER_START = \"evidence:\"\n",
        "    # find(f'\"{self.generation_marker}\"')\n",
        "    start_parsing = response.find(f\"{PARSER_START}\")\n",
        "    if (start_parsing != -1):\n",
        "        reponse = response[start_parsing:]\n",
        "\n",
        "    LABEL_START_MARKER = \"label:\"\n",
        "    DOC_START_MARKER = \"doc:\"\n",
        "    STATE_START_MARKER = \"state:\"\n",
        "\n",
        "    label_start = response.find(LABEL_START_MARKER)\n",
        "    doc_start = response.find(DOC_START_MARKER)\n",
        "    state_start = response.find(STATE_START_MARKER)\n",
        "\n",
        "    evidence = response[:label_start]\n",
        "    evidence = evidence.split(\"evidence: \")[-1].strip()\n",
        "    label = response[label_start:doc_start]\n",
        "    label = re.split(r\"label:\\n*\\t*\\s*\", label)[-1].strip()\n",
        "    doc = response[doc_start:state_start]\n",
        "    doc = re.split(r\"doc:\\\"*\\n*\\t*\\s*\", doc)[-1].split(\"或\")[0].strip()\n",
        "    state = response[state_start:]\n",
        "\n",
        "    state = re.split(r\"state:\\n*\\t*\\s*\", state)[-1].split(\"\\n\")[0]\n",
        "\n",
        "    return evidence, label, doc, state\n"
      ],
      "metadata": {
        "id": "6sQKzUMNeP3I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def refreshState():\n",
        "\t\tlabel=\"None\"\n",
        "\t\tdoc=\"None\"\n",
        "\t\tstate=\"None\"\n",
        "\n",
        "\t\treturn label,doc,state"
      ],
      "metadata": {
        "id": "wkVbocuNQJ3J"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    print(\"***************************************************\")\n",
        "    claim=claim=input(\"請輸入一個陳述句:\")\n",
        "\n",
        "    if claim=='q':\n",
        "        break\n",
        "\n",
        "    text=\"KeyError\"\n",
        "    evidence=\"\"\n",
        "\n",
        "    while(text==\"KeyError\"):\n",
        "        keywords=doc_chain.run(claim)\n",
        "        keyword=keywordParser(keywords)\n",
        "        text=get_wiki_text(wiki,keyword)\n",
        "\n",
        "    while True:\n",
        "        label,doc,state=refreshState()\n",
        "        response=agent.run(claim=claim,evidence=evidence,keyword=keyword,text=text)\n",
        "        print(\"\\033[91m\\033[1m\" + \"response:\\n\",response, \"\\033[0m\\033[0m\")\n",
        "        evidence,label,doc,state=getAction(response)\n",
        "\n",
        "        IS_FINISH=(state==\"finish\" and doc==\"None\")\n",
        "        #NOT_ENOUGH_INFO=(label==\"not enough info\" and (doc==\"None\" or state==\"finish\"))\n",
        "        #NOT_ENOUGH_INFO=(label==\"not enough info\" and doc!=keyword\n",
        "        #IS_CONTINUE=(state==\"continue\" and doc!=\"None\")\n",
        "        NOT_ENOUGH_INFO=(label==\"not enough info\" or doc==keyword)\n",
        "        IS_CONTINUE=(state==\"continue\")\n",
        "\n",
        "        if IS_FINISH:\n",
        "            answer=claimRetrieval.run(claim=claim,evidence=evidence)\n",
        "            print(\"\\033[96m\\033[1m\" ,answer, \"\\033[0m\\033[0m\")\n",
        "            break\n",
        "\n",
        "        if NOT_ENOUGH_INFO:\n",
        "            print(\"\\033[90m\\033[1m\" + \"claim:\",claim,\"is NOT ENOUGH INFO\" \"\\033[0m\\033[0m\")\n",
        "            break\n",
        "\n",
        "        if IS_CONTINUE:\n",
        "            if wiki[doc]!=\"KeyError\":\n",
        "                text=wiki[doc]\n",
        "            #evidence=evidence\n",
        "            continue\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "00kBB3Diet93",
        "outputId": "18670dca-4fb7-4094-c741-c06bcf66496c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***************************************************\n",
            "請輸入一個陳述句:中國人馮端的主要研究方向是原子排列具有隨機性結構的晶體。\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new EvidenceRetrivalBot chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "陳述句:中國人馮端的主要研究方向是原子排列具有隨機性結構的晶體。\n",
            "\n",
            "已檢索的證據句:\"\"\n",
            "\n",
            "目前的文章為\"馮端\":馮端  ， 男 ， 原籍浙江紹興 ， 生於江蘇蘇州 ， 中國物理學家 ， 中國科學院院士 ， 主要研究領域爲固體物理學 。\n",
            "\n",
            "上述的陳述句有可能是真實的也有可能虛假的，而當前檢索的文章為可靠的資訊來源。讓我們來完成事實查核任務，依照以下步驟先進行思考:\n",
            "\n",
            "思考:是否可以文章中檢索出與陳述句相關的句子作為證據句來\"支持或反駁\"陳述句?如果證據句無法說明陳述句的真偽，根據缺失的信息從句子當中找出關鍵字作為下一步需要檢索的頁面名稱。\n",
            "\n",
            "完成以上步驟並提供思考過程。\n",
            "\n",
            "根據思考的內容整理成\"純文字\"，依據對應指示填入答案。以下是內容整理的樣式以及相對應的指示:\n",
            "\n",
            "evidence:輸出從文章中檢索的句子並且不要使用: 數字 以及 · 以及 \n",
            " 來區分每一句話。\n",
            "label:如果下一步檢索的文章名稱與當前文章相同，表示無法再找出缺失的信息，輸出\"Not Enough Info\"，否則無論是否需要再進行檢索文章，均為\"None\"\n",
            "doc:根據找出得關鍵字決定下一步需要檢索的文章名稱。如果不再進行檢索其他文章則為\"None\"\n",
            "state:如果需要繼續檢索其他文章則狀態為\"continue\"，如果已經找齊所有證據句則狀態為\"finish\"\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[91m\u001b[1mresponse:\n",
            " evidence: None\n",
            "label: None\n",
            "doc: None\n",
            "state: finish \u001b[0m\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new MyChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "根據| |前後的雙引號\"\"包圍的一組證據句以及\"\"包圍的陳述句，按照以下步驟以及要求，選擇使用 支持/反駁/資訊不足 來完成三分類任務\n",
            "\n",
            "一、從陳述句和證據句當中抽取出可以互相對應對應的單詞。\n",
            "\n",
            "二、接著檢出每一個對應的單詞，如果含義無法成功匹配，則視為關鍵詞。\n",
            "\n",
            "三、檢查證據句的關鍵詞是否有蘊含陳述句當中的關鍵詞，如果關鍵詞的含義不等同則視為反駁。不等同的狀況請參考以下但不限於以下範例：\"含義明顯衝突、無法經由推論得出結論、蘊含關係不明確。\"參考以上範例根據提供的句子做出合理的判斷。\n",
            "\n",
            "\"None\"| |\"中國人馮端的主要研究方向是原子排列具有隨機性結構的晶體。\"\n",
            "\n",
            "根據上下文| |應填入 支持/反駁/資訊不足 ？\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[96m\u001b[1m 資訊不足 \u001b[0m\u001b[0m\n",
            "***************************************************\n",
            "請輸入一個陳述句:中國人馮端的主要研究方向是原子排列具有隨機性結構的晶體。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new EvidenceRetrivalBot chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "陳述句:中國人馮端的主要研究方向是原子排列具有隨機性結構的晶體。\n",
            "\n",
            "已檢索的證據句:\"\"\n",
            "\n",
            "目前的文章為\"馮端\":馮端  ， 男 ， 原籍浙江紹興 ， 生於江蘇蘇州 ， 中國物理學家 ， 中國科學院院士 ， 主要研究領域爲固體物理學 。\n",
            "\n",
            "上述的陳述句有可能是真實的也有可能虛假的，而當前檢索的文章為可靠的資訊來源。讓我們來完成事實查核任務，依照以下步驟先進行思考:\n",
            "\n",
            "思考:是否可以文章中檢索出與陳述句相關的句子作為證據句來\"支持或反駁\"陳述句?如果證據句無法說明陳述句的真偽，根據缺失的信息從句子當中找出關鍵字作為下一步需要檢索的頁面名稱。\n",
            "\n",
            "完成以上步驟並提供思考過程。\n",
            "\n",
            "根據思考的內容整理成\"純文字\"，依據對應指示填入答案。以下是內容整理的樣式以及相對應的指示:\n",
            "\n",
            "evidence:輸出從文章中檢索的句子並且不要使用: 數字 以及 · 以及 \n",
            " 來區分每一句話。\n",
            "label:如果下一步檢索的文章名稱與當前文章相同，表示無法再找出缺失的信息，輸出\"Not Enough Info\"，否則無論是否需要再進行檢索文章，均為\"None\"\n",
            "doc:根據找出得關鍵字決定下一步需要檢索的文章名稱。如果不再進行檢索其他文章則為\"None\"\n",
            "state:如果需要繼續檢索其他文章則狀態為\"continue\"，如果已經找齊所有證據句則狀態為\"finish\"\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[91m\u001b[1mresponse:\n",
            " evidence: None\n",
            "label: None\n",
            "doc: None\n",
            "state: finish \u001b[0m\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new MyChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "根據| |前後的雙引號\"\"包圍的一組證據句以及\"\"包圍的陳述句，按照以下步驟以及要求，選擇使用 支持/反駁/資訊不足 來完成三分類任務\n",
            "\n",
            "一、從陳述句和證據句當中抽取出可以互相對應對應的單詞。\n",
            "\n",
            "二、接著檢出每一個對應的單詞，如果含義無法成功匹配，則視為關鍵詞。\n",
            "\n",
            "三、檢查證據句的關鍵詞是否有蘊含陳述句當中的關鍵詞，如果關鍵詞的含義不等同則視為反駁。不等同的狀況請參考以下但不限於以下範例：\"含義明顯衝突、無法經由推論得出結論、蘊含關係不明確。\"參考以上範例根據提供的句子做出合理的判斷。\n",
            "\n",
            "\"None\"| |\"中國人馮端的主要研究方向是原子排列具有隨機性結構的晶體。\"\n",
            "\n",
            "根據上下文| |應填入 支持/反駁/資訊不足 ？\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-5qGjvC0WBRN73RMlAjaYOEMG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c43a49f7fdf3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIS_FINISH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0manswer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclaimRetrieval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclaim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\033[96m\\033[1m\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\033[0m\\033[0m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             outputs = (\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    450\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             ]\n\u001b[0;32m--> 582\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    583\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             output = (\n\u001b[0;32m--> 475\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    476\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_chat_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m         full_response = completion_with_retry(\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoSleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_for_next_attempt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdo\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/nap.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmocked\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \"\"\"\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}