{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### setup"
      ],
      "metadata": {
        "id": "S09Sl1kGLn8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive') # 此處需要登入google帳號\n",
        "\n",
        "!git clone https://github.com/IKMLab/NCKU-AICUP2023-baseline\n",
        "%cd NCKU-AICUP2023-baseline\n",
        "!cp -r /content/gdrive/MyDrive/aicup-2023-plain/* ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPNnt2uEQKgV",
        "outputId": "c1ec0247-d627-4935-87d9-d01e7701c452"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Cloning into 'NCKU-AICUP2023-baseline'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 146 (delta 34), reused 30 (delta 22), pack-reused 101\u001b[K\n",
            "Receiving objects: 100% (146/146), 82.83 KiB | 831.00 KiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "/content/NCKU-AICUP2023-baseline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIUoNn0YM7SU",
        "outputId": "fb670954-22a2-4f04-cd1f-074d0e094143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install -q ipywidgets pandarallel pandas scikit-learn tqdm opencc wikipedia\n",
        "!pip install -q pandas wikipedia\n",
        "!pip -q install openai langchain #huggingface_hub\n",
        "#!pip install dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# 3rd party libs\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "\n",
        "#from dotenv import dotenv_values\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "#os.environ['OPENAI_API_KEY']=dotenv.dotenv_values(.env)[\"OPENAI_API_KEY\"]\n",
        "wikipedia.set_lang(\"zh\")\n",
        "\n",
        "import langchain\n",
        "langchain.debug=False\n"
      ],
      "metadata": {
        "id": "Pibak_jWNEbr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### functions(wiki...)"
      ],
      "metadata": {
        "id": "tmsC7j7sLdPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf8\") as json_file:\n",
        "        json_list = list(json_file)\n",
        "    return [json.loads(json_str) for json_str in json_list]\n",
        "\n",
        "def wiki_database(path):\n",
        "\t\tdatabase=pd.concat(\n",
        "        [pd.DataFrame(load_json(file)) for file in Path(path).glob(\"*.jsonl\")]\n",
        "    )\n",
        "\t\treturn dict(zip(database[\"id\"].tolist(),database[\"text\"].tolist()))\n",
        "\n",
        "def get_wiki_text(wiki,key_word):\n",
        "    try:\n",
        "      text=wiki[key_word]\n",
        "    except KeyError:\n",
        "      text=\"KeyError\"\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "DWgEnUIBPmGW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WIKI_PATH=\"data/wiki-pages\"\n",
        "\n",
        "wiki=wiki_database(WIKI_PATH)"
      ],
      "metadata": {
        "id": "qVlFZ_eQqcl_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build chains"
      ],
      "metadata": {
        "id": "ubyVZcs1NpnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models.openai import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "#text-davinci-003\n",
        "MODEL_NAME=\"gpt-3.5-turbo-16k-0613\"\n",
        "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=1.2, max_tokens = 4096)\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n"
      ],
      "metadata": {
        "id": "08DLt9OzNHH1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyChain(LLMChain):\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm_and_prompt(cls,llm,prompt,verbose=False):\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ],
      "metadata": {
        "id": "wNZUsAejHC8n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EvidenceRetrivalBot(LLMChain):\n",
        "    \"\"\"implenment agent concept via chain class\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm_and_prompt(cls,llm,prompt,verbose=False):\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ],
      "metadata": {
        "id": "R_LaL3bFOhtH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "doc_retrieval_template = \"\"\"\n",
        "你是一個看過維基百科內容的機器人，讓我們來預測以下提供的陳述句可能的出處。請從陳述句中選擇一個單字作為檢索的關鍵字。\n",
        "\n",
        "claim:{claim}\n",
        "\n",
        "按照以下格式返回預測的關鍵詞\n",
        "keyword:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"claim\"],\n",
        "    template=doc_retrieval_template,\n",
        ")\n",
        "\n",
        "doc_chain=MyChain.from_llm_and_prompt(llm=llm,prompt=prompt)\n"
      ],
      "metadata": {
        "id": "oaUakMO5Nv0c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evidence_selection_template = \"\"\"\n",
        "陳述句:{claim}\n",
        "以檢索的證據句:{evidence}\n",
        "文章:{text}\n",
        "\n",
        "目前正在檢索的文章是:{keyword}\n",
        "\n",
        "現在要完成的是事實查核任務：從文章中找出與陳述句相關的句子，也有可能文章中不存在相關的句子。如果文章中的句子沒有陳述句的完整信息，請依照缺失的信息根據句子中的\"關鍵詞\"決定下一步需要檢索的文章(文章名稱必須是維基百科頁面，名詞)在思考下一步需要檢索的文章請記得不要是當前檢索的文章(當前檢索的文章名稱是什麼？)。如果思考證據句缺失的信息無法經由檢索其他文章中的句子完成任務則標籤為\"資訊不足\"\n",
        "\n",
        "先完成以上步驟給出思考過程。最後整理答案按照以下格式根據指示輸出：\n",
        "evidence:輸出所有證據句，包含已檢索的證據句以及從文章中檢索得到的證據句。並且不要使用: 數字 以及 · 以及 \\n 來區分每一句話。\n",
        "doc:下一步需要檢索的文章。如果不再進行檢索其他文章則為\"None\"。如果缺失的信息無法經由檢索其他文章中的句子完成任務則標籤為\"Not enough info\"。\n",
        "state:如果需要繼續檢索其他文章則狀態為\"continue\"，如果已經找齊所有證據句則狀態為\"finish\"。\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "human_message_prompt = HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=evidence_selection_template,\n",
        "            input_variables=[\"claim\",\"evidence\",\"text\",\"keyword\"],\n",
        "        )\n",
        "    )\n",
        "agent_planning = ChatPromptTemplate.from_messages([human_message_prompt,SystemMessage(\n",
        "        content=\"You are ChatGPT,a large language model trained by OpenAI.\\nKnowledge cutoff:2021-09\\nCurrent data:[current date]\\n\"\n",
        "    )])\n",
        "\n",
        "agent=EvidenceRetrivalBot.from_llm_and_prompt(llm=llm, prompt=agent_planning,verbose=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MPzxSj83Oumw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim_retrieval_template = \"\"\"\n",
        "證據句可能支持或反駁陳述句，以下提供的證據句與陳述句存在微小的差異，而只要有一點意思的的差異就會導致證據句\"反駁\"陳述句。按照以下步驟決定證據句支持或反駁陳述句:\n",
        "\n",
        "接下來請先找出證據句與陳述句之間的決定句子意思的關鍵字，接著仔細思考兩者是否存在差異：\"含義明顯衝突\"、\"無法經由推論得出結論\"、\"蘊含關係不明確\"。若有上述情況發生則視為反駁。\n",
        "\n",
        "請仔細檢查以上的狀況是否有發生，如果不存在則視為支持\n",
        "\n",
        "證據句:{evidence}\n",
        "陳述句:{claim}\n",
        "\n",
        "按照以上步驟決定證據句是支持還是反駁陳述句\n",
        "\"\"\"\n",
        "human_message_prompt = HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            input_variables=[\"claim\",\"evidence\"],\n",
        "            template=claim_retrieval_template,\n",
        "        )\n",
        "    )\n",
        "claim_retrieval = ChatPromptTemplate.from_messages([human_message_prompt,SystemMessage(\n",
        "        content=\"You are ChatGPT,a large language model trained by OpenAI.\\nKnowledge cutoff:2021-09\\nCurrent data:[current date]\\n\"\n",
        "    )])\n",
        "\n",
        "claimRetrieval=MyChain.from_llm_and_prompt(llm=llm, prompt=claim_retrieval,verbose=True)\n"
      ],
      "metadata": {
        "id": "mqS9JSqiQvme"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main"
      ],
      "metadata": {
        "id": "Ke342oxSfiOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "font ref:https://blog.csdn.net/c_lanxiaofang/article/details/126107796"
      ],
      "metadata": {
        "id": "9YEn1L-9Kavx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def keywordParser(keywords):\n",
        "    #\\s | Matches whitespace characters, which include the \\t, \\n, \\r, and space characters.\n",
        "    PATTERN=re.compile(r\"(K|k)eywords?\\s*:?\\s*\\'*\\\"*\")\n",
        "    keyword=re.split(PATTERN,keywords)[-1]\n",
        "    return keyword\n",
        "\n",
        "def getAction(response):\n",
        "    PARSER_START = \"evidence:\"\n",
        "    # find(f'\"{self.generation_marker}\"')\n",
        "    start_parsing = response.find(f\"{PARSER_START}\")\n",
        "    if (start_parsing != -1):\n",
        "        reponse = response[start_parsing:]\n",
        "\n",
        "    #LABEL_START_MARKER = \"label:\"\n",
        "    DOC_START_MARKER = \"doc:\"\n",
        "    STATE_START_MARKER = \"state:\"\n",
        "\n",
        "    #label_start = response.find(LABEL_START_MARKER)\n",
        "    doc_start = response.find(DOC_START_MARKER)\n",
        "    state_start = response.find(STATE_START_MARKER)\n",
        "\n",
        "    evidence = response[:doc_start]\n",
        "    evidence = re.split(r\"evidence:\\n*\\t*\\s*\", evidence)[-1].strip()\n",
        "    doc = response[doc_start:state_start]\n",
        "    doc = re.split(r\"doc:\\\"*\\n*\\t*\\s*\", doc)[-1].split(\"或\")[0].strip()\n",
        "    state = response[state_start:]\n",
        "    state = re.split(r\"state:\\n*\\t*\\s*\", state)[-1].strip()\n",
        "\n",
        "    return evidence,doc, state\n"
      ],
      "metadata": {
        "id": "6sQKzUMNeP3I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def refreshState():\n",
        "\t\tlabel=\"None\"\n",
        "\t\tdoc=\"None\"\n",
        "\t\tstate=\"None\"\n",
        "\t\treturn label,doc,state"
      ],
      "metadata": {
        "id": "wkVbocuNQJ3J"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Ct6KI-lfjY-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"***************************************************\")\n",
        "claim=claim=input(\"請輸入一個陳述句:\")\n",
        "\n",
        "text=\"KeyError\"\n",
        "evidence=\"需要從以下文章中檢索\"\n",
        "\n",
        "while(text==\"KeyError\"):\n",
        "    keywords=doc_chain.run(claim)\n",
        "    keyword=keywordParser(keywords)\n",
        "    text=get_wiki_text(wiki,keyword)\n",
        "\n",
        "while True:\n",
        "    label,doc,state=refreshState()\n",
        "    response=agent.run(claim=claim,evidence=evidence,keyword=keyword,text=text)\n",
        "    print(\"\\033[91m\\033[1m\" ,response, \"\\033[0m\\033[0m\")\n",
        "    evidence,doc,state=getAction(response)\n",
        "    #print(\"evidence:\",evidence,\"\\ndoc:\",doc)\n",
        "\n",
        "    IS_FINISH=(state==\"finish\" and doc==\"None\")\n",
        "    NOT_ENOUGH_INFO=(doc==\"Not enough info\" or (doc==keyword and state==\"finish\"))\n",
        "    IS_CONTINUE=(state==\"continue\")\n",
        "\n",
        "    if IS_FINISH:\n",
        "        answer=claimRetrieval.run(claim=claim,evidence=evidence)\n",
        "        print(\"\\033[96m\\033[1m\" ,answer, \"\\033[0m\\033[0m\")\n",
        "        break\n",
        "\n",
        "    elif NOT_ENOUGH_INFO:\n",
        "        print(\"\\033[90m\\033[1m\" + \"claim:\",claim,\"is NOT ENOUGH INFO\" \"\\033[0m\\033[0m\")\n",
        "        break\n",
        "\n",
        "    elif IS_CONTINUE:\n",
        "        if get_wiki_text(wiki,doc)!=\"KeyError\":\n",
        "            text=wiki[doc]\n",
        "        #evidence=evidence\n",
        "        continue\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00kBB3Diet93",
        "outputId": "5b685047-2417-460a-be9f-897aa105628b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***************************************************\n",
            "請輸入一個陳述句:尼克·貝爾格在2004年前往中東尋找生意商機\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new EvidenceRetrivalBot chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: \n",
            "陳述句:尼克·貝爾格在2004年前往中東尋找生意商機\n",
            "以檢索的證據句:需要從以下文章中檢索\n",
            "文章:尼古拉斯 · 伊萬 · “ 尼克 ” · 貝爾格 （ Nicholas Evan \" Nick \" Berg ) ， 是一位美國賓夕法尼亞州費城市郊的美國商人 。 他在2004年3月前往伊拉克尋找通訊生意商機 ， 自2004年4月9日失蹤 。 一家與 “ 基地組織 ” 有關的伊斯蘭網站將他被血腥地斬首的錄像播放來報復美英聯軍虐待伊拉克戰俘事件 。 據報他死亡時爲26歲 ， 他的無頭屍體於2004年5月8日被掛在巴格達的美軍基地附近一個高速公路天橋 ， 滿布刀痕的頭部放在屍體下 。\n",
            "\n",
            "目前正在檢索的文章是:尼克·貝爾格\n",
            "\n",
            "現在要完成的是事實查核任務：從文章中找出與陳述句相關的句子，也有可能文章中不存在相關的句子。如果文章中的句子沒有陳述句的完整信息，請依照缺失的信息根據句子中的\"關鍵詞\"決定下一步需要檢索的文章(文章名稱必須是維基百科頁面，名詞)在思考下一步需要檢索的文章請記得不要是當前檢索的文章(當前檢索的文章名稱是什麼？)。如果思考證據句缺失的信息無法經由檢索其他文章中的句子完成任務則標籤為\"資訊不足\"\n",
            "\n",
            "先完成以上步驟給出思考過程。最後整理答案按照以下格式根據指示輸出：\n",
            "evidence:輸出所有證據句，包含已檢索的證據句以及從文章中檢索得到的證據句。並且不要使用: 數字 以及 · 以及 \n",
            " 來區分每一句話。\n",
            "doc:下一步需要檢索的文章。如果不再進行檢索其他文章則為\"None\"。如果缺失的信息無法經由檢索其他文章中的句子完成任務則標籤為\"Not enough info\"。\n",
            "state:如果需要繼續檢索其他文章則狀態為\"continue\"，如果已經找齊所有證據句則狀態為\"finish\"。\n",
            "\n",
            "System: You are ChatGPT,a large language model trained by OpenAI.\n",
            "Knowledge cutoff:2021-09\n",
            "Current data:[current date]\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[91m\u001b[1m Current evidence:\n",
            "1. 尼克·貝爾格在2004年前往中東尋找生意商機\n",
            "\n",
            "Evidence retrieved from the article:\n",
            "- 他在2004年3月前往伊拉克尋找通訊生意商機\n",
            "\n",
            "Based on the evidence found in the article, we can confirm that Nick Berg went to Iraq in March 2004 to seek business opportunities in the communication sector. \n",
            "\n",
            "Next, we need to find out whether the evidence is complete or if there is any additional information missing. To do so, we will search for other relevant sentences in the same article or another Wikipedia page.\n",
            "\n",
            "Since the article provides the necessary information, there is no need to search for additional sentences or other articles. Therefore, the response will be as follows:\n",
            "\n",
            "evidence:\n",
            "- 他在2004年3月前往伊拉克尋找通訊生意商機\n",
            "\n",
            "doc: None\n",
            "state: finish \u001b[0m\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new MyChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: \n",
            "證據句可能支持或反駁陳述句，以下提供的證據句與陳述句存在微小的差異，而只要有一點意思的的差異就會導致證據句\"反駁\"陳述句。按照以下步驟決定證據句支持或反駁陳述句:\n",
            "\n",
            "接下來請先找出證據句與陳述句之間的決定句子意思的關鍵字，接著仔細思考兩者是否存在差異：\"含義明顯衝突\"、\"無法經由推論得出結論\"、\"蘊含關係不明確\"。若有上述情況發生則視為反駁。\n",
            "\n",
            "請仔細檢查以上的狀況是否有發生，如果不存在則視為支持\n",
            "\n",
            "證據句:- 他在2004年3月前往伊拉克尋找通訊生意商機\n",
            "陳述句:尼克·貝爾格在2004年前往中東尋找生意商機\n",
            "\n",
            "按照以上步驟決定證據句是支持還是反駁陳述句\n",
            "\n",
            "System: You are ChatGPT,a large language model trained by OpenAI.\n",
            "Knowledge cutoff:2021-09\n",
            "Current data:[current date]\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[96m\u001b[1m 進行評估時，首先需要確認關鍵字或詞語。在這個案例中，關鍵字是「2004年」、「尋找」、「伊拉克」和「中東」。\n",
            "\n",
            "接下來，進行證據句和陳述句之間的比較：\n",
            "\n",
            "證據句：他在2004年3月前往伊拉克尋找通訊生意商機。\n",
            "陳述句：尼克·貝爾格在2004年前往中東尋找生意商機。\n",
            "\n",
            "通過檢查關鍵詞，可以發現以下幾點差異：\n",
            "\n",
            "1.「伊拉克」與「中東」：證據句指明他前往伊拉克，而陳述句指稱他前往整個中東地區。\n",
            "\n",
            "基於以上差異，可以得出一個結論：證據句支持陳述句，因為他前往伊拉克就包含了他前往中東的情況，伊拉克是中東地區的一部分。\n",
            "\n",
            "因此，證據句支持陳述句。 \u001b[0m\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}