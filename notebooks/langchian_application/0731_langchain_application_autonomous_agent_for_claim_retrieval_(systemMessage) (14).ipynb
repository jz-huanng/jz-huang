{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S09Sl1kGLn8n"
      },
      "source": [
        "### setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "反駁了陳述句,證據句沒找齊"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPNnt2uEQKgV",
        "outputId": "24de3fce-f7e9-42e5-837b-8c92bfb0e967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Cloning into 'NCKU-AICUP2023-baseline'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 146 (delta 34), reused 30 (delta 22), pack-reused 101\u001b[K\n",
            "Receiving objects: 100% (146/146), 82.83 KiB | 4.60 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n",
            "/content/NCKU-AICUP2023-baseline\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive') # 此處需要登入google帳號\n",
        "\n",
        "!git clone https://github.com/IKMLab/NCKU-AICUP2023-baseline\n",
        "%cd NCKU-AICUP2023-baseline\n",
        "!cp -r /content/gdrive/MyDrive/aicup-2023-plain/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIUoNn0YM7SU",
        "outputId": "e875f79e-4e71-42f0-ccf1-80e59ecbba28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install -q ipywidgets pandarallel pandas scikit-learn tqdm opencc wikipedia\n",
        "!pip install -q pandas wikipedia\n",
        "!pip -q install openai langchain #huggingface_hub\n",
        "#!pip install dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Pibak_jWNEbr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# 3rd party libs\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "\n",
        "#from dotenv import dotenv_values\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "#os.environ['OPENAI_API_KEY']=dotenv.dotenv_values(.env)[\"OPENAI_API_KEY\"]\n",
        "wikipedia.set_lang(\"zh\")\n",
        "\n",
        "import langchain\n",
        "langchain.debug=False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmsC7j7sLdPR"
      },
      "source": [
        "### functions(wiki...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DWgEnUIBPmGW"
      },
      "outputs": [],
      "source": [
        "def load_json(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf8\") as json_file:\n",
        "        json_list = list(json_file)\n",
        "    return [json.loads(json_str) for json_str in json_list]\n",
        "\n",
        "def wiki_database(path):\n",
        "\t\tdatabase=pd.concat(\n",
        "        [pd.DataFrame(load_json(file)) for file in Path(path).glob(\"*.jsonl\")]\n",
        "    )\n",
        "\t\treturn dict(zip(database[\"id\"].tolist(),database[\"text\"].tolist()))\n",
        "\n",
        "def get_wiki_text(wiki,key_word):\n",
        "    try:\n",
        "      text=wiki[key_word]\n",
        "    except KeyError:\n",
        "      text=\"KeyError\"\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qVlFZ_eQqcl_"
      },
      "outputs": [],
      "source": [
        "WIKI_PATH=\"data/wiki-pages\"\n",
        "\n",
        "wiki=wiki_database(WIKI_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubyVZcs1NpnM"
      },
      "source": [
        "### Build chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "08DLt9OzNHH1"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models.openai import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "#text-davinci-003\n",
        "MODEL_NAME=\"gpt-3.5-turbo-16k-0613\"\n",
        "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=1.2, max_tokens = 4096)\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wNZUsAejHC8n"
      },
      "outputs": [],
      "source": [
        "class MyChain(LLMChain):\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm_and_prompt(cls,llm,prompt,verbose=False):\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "R_LaL3bFOhtH"
      },
      "outputs": [],
      "source": [
        "class EvidenceRetrivalBot(LLMChain):\n",
        "    \"\"\"implenment agent concept via chain class\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm_and_prompt(cls,llm,prompt,verbose=False):\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oaUakMO5Nv0c"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "doc_retrieval_template = \"\"\"\n",
        "你是一個看過維基百科內容的機器人，讓我們來預測以下提供的陳述句可能的出處。請從陳述句中選擇一個單字作為檢索的關鍵字。\n",
        "\n",
        "claim:{claim}\n",
        "\n",
        "按照以下格式返回預測的關鍵詞\n",
        "keyword:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"claim\"],\n",
        "    template=doc_retrieval_template,\n",
        ")\n",
        "\n",
        "doc_chain=MyChain.from_llm_and_prompt(llm=llm,prompt=prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MPzxSj83Oumw"
      },
      "outputs": [],
      "source": [
        "evidence_selection_template = \"\"\"\n",
        "陳述句:{claim}\n",
        "以檢索的證據句:{evidence}\n",
        "文章:{text}\n",
        "\n",
        "目前正在檢索的文章是:{keyword}\n",
        "\n",
        "現在要完成的是事實查核任務：從文章中找出與陳述句相關的句子，也有可能文章中不存在相關的句子。如果文章中的句子沒有陳述句的完整信息，請依照缺失的信息根據句子中的\"關鍵詞\"決定下一步需要檢索的文章(文章名稱必須是維基百科頁面，名詞)在思考下一步需要檢索的文章請記得不要是當前檢索的文章(當前檢索的文章名稱是什麼？)。如果思考證據句缺失的信息無法經由檢索其他文章中的句子完成任務則標籤為\"資訊不足\"\n",
        "\n",
        "先完成以上步驟給出思考過程。最後整理答案按照以下格式根據指示輸出：\n",
        "evidence:輸出所有證據句，包含已檢索的證據句以及從文章中檢索得到的證據句。並且不要使用: 數字 以及 · 以及 \\n 來區分每一句話。\n",
        "doc:下一步需要檢索的文章。如果不再進行檢索其他文章則為\"None\"。如果缺失的信息無法經由檢索其他文章中的句子完成任務則標籤為\"Not enough info\"。\n",
        "state:如果需要繼續檢索其他文章則狀態為\"continue\"，如果已經找齊所有證據句則狀態為\"finish\"。\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "human_message_prompt = HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=evidence_selection_template,\n",
        "            input_variables=[\"claim\",\"evidence\",\"text\",\"keyword\"],\n",
        "        )\n",
        "    )\n",
        "agent_planning = ChatPromptTemplate.from_messages([human_message_prompt,SystemMessage(\n",
        "        content=\"You are ChatGPT,a large language model trained by OpenAI.\\nKnowledge cutoff:2021-09\\nCurrent data:[current date]\\n\"\n",
        "    )])\n",
        "\n",
        "agent=EvidenceRetrivalBot.from_llm_and_prompt(llm=llm, prompt=agent_planning,verbose=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mqS9JSqiQvme"
      },
      "outputs": [],
      "source": [
        "claim_retrieval_template = \"\"\"\n",
        "證據句可能支持或反駁陳述句，以下提供的證據句與陳述句存在微小的差異，而只要有一點意思的的差異就會導致證據句\"反駁\"陳述句。按照以下步驟決定證據句支持或反駁陳述句:\n",
        "\n",
        "接下來請先找出證據句與陳述句之間的決定句子意思的關鍵字，接著仔細思考兩者是否存在差異：\"含義明顯衝突\"、\"無法經由推論得出結論\"、\"蘊含關係不明確\"。若有上述情況發生則視為反駁。\n",
        "\n",
        "請仔細檢查以上的狀況是否有發生，如果不存在則視為支持\n",
        "\n",
        "證據句:{evidence}\n",
        "陳述句:{claim}\n",
        "\n",
        "按照以上步驟決定證據句是支持還是反駁陳述句\n",
        "\"\"\"\n",
        "human_message_prompt = HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            input_variables=[\"claim\",\"evidence\"],\n",
        "            template=claim_retrieval_template,\n",
        "        )\n",
        "    )\n",
        "claim_retrieval = ChatPromptTemplate.from_messages([human_message_prompt,SystemMessage(\n",
        "        content=\"You are ChatGPT,a large language model trained by OpenAI.\\nKnowledge cutoff:2021-09\\nCurrent data:[current date]\\n\"\n",
        "    )])\n",
        "\n",
        "claimRetrieval=MyChain.from_llm_and_prompt(llm=llm, prompt=claim_retrieval,verbose=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke342oxSfiOp"
      },
      "source": [
        "### main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YEn1L-9Kavx"
      },
      "source": [
        "font ref:https://blog.csdn.net/c_lanxiaofang/article/details/126107796"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6sQKzUMNeP3I"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def keywordParser(keywords):\n",
        "    #\\s | Matches whitespace characters, which include the \\t, \\n, \\r, and space characters.\n",
        "    PATTERN=re.compile(r\"(K|k)eywords?\\s*:?\\s*\\'*\\\"*\")\n",
        "    keyword=re.split(PATTERN,keywords)[-1]\n",
        "    return keyword\n",
        "\n",
        "def getAction(response):\n",
        "    PARSER_START = \"evidence:\"\n",
        "    # find(f'\"{self.generation_marker}\"')\n",
        "    start_parsing = response.find(f\"{PARSER_START}\")\n",
        "    if (start_parsing != -1):\n",
        "        reponse = response[start_parsing:]\n",
        "\n",
        "    #LABEL_START_MARKER = \"label:\"\n",
        "    DOC_START_MARKER = \"doc:\"\n",
        "    STATE_START_MARKER = \"state:\"\n",
        "\n",
        "    #label_start = response.find(LABEL_START_MARKER)\n",
        "    doc_start = response.find(DOC_START_MARKER)\n",
        "    state_start = response.find(STATE_START_MARKER)\n",
        "\n",
        "    evidence = response[:doc_start]\n",
        "    evidence = re.split(r\"evidence:\\n*\\t*\\s*\", evidence)[-1].strip()\n",
        "    doc = response[doc_start:state_start]\n",
        "    doc = re.split(r\"doc:\\\"*\\n*\\t*\\s*\", doc)[-1].split(\"或\")[0].strip()\n",
        "    state = response[state_start:]\n",
        "    state = re.split(r\"state:\\n*\\t*\\s*\", state)[-1].strip()\n",
        "\n",
        "    return evidence,doc, state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wkVbocuNQJ3J"
      },
      "outputs": [],
      "source": [
        "def refreshState():\n",
        "\t\tlabel=\"None\"\n",
        "\t\tdoc=\"None\"\n",
        "\t\tstate=\"None\"\n",
        "\t\treturn label,doc,state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct6KI-lfjY-u"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00kBB3Diet93",
        "outputId": "020dd452-554c-42dc-fa97-d245a9dd8f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***************************************************\n",
            "請輸入一個陳述句:德州儀器於1950年代初開始研究一種類似於閥門的固態半導體元件，製造了世界上第一個商用硅晶體管。\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new EvidenceRetrivalBot chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: \n",
            "陳述句:德州儀器於1950年代初開始研究一種類似於閥門的固態半導體元件，製造了世界上第一個商用硅晶體管。\n",
            "以檢索的證據句:需要從以下文章中檢索\n",
            "文章:Infobox company | name = 德州儀器公司 | name_en = Texas Instruments Incorporated | company_logo = TexasInstruments - Logo.svg | company_type = 上市公司 | traded_as = | company_slogan = Your Passion. Our Technology. Student Success. | foundation = 1930年 （ 作爲GSI ） 1947年 （ 作爲德州儀器 ） | location = 德克薩斯州達拉斯 | key_people = Rich Templeton （ 總裁 、 首席執行官 ） Kevin March （ 首席財務官 ） Brian Bonner （ 首席信息官 ） | industry = 半導體 、 電子 | products = 集成電路 、 模擬電路 、 數字信號處理器 、 數字光處理 、 射頻識別 、 計算器等 | revenue = （ 2019 ） | operating_income = （ 2019 ） | net_income = （ 2019 ） | assets = （ 2019 ） | equity = （ 2019 ） | num_employees = 29,888 （ 2019 ） | homepage = 德州儀器 （ Texas Instruments ） ， 簡稱德儀 （ TI ） ， 是一家美國跨國科技公司 ， 總部位於德克薩斯州的達拉斯 ， 以開發 、 製造 、 銷售半導體和計算器技術聞名於世 ， 主要從事數字信號處理與模擬電路方面的研究 、 製造和銷售 。 它在25個國家有製造 、 設計或者銷售機構 。 根據2021年IC Insights統計 ， 德州儀器是世界第九大半導體制造商 ； 曾經是行動電話的第二大芯片供應商 ， 僅次於高通 ； 同時也是在世界範圍內的第一大數字信號處理器 （ DSP ） 和模擬半導體元件的製造商 ， 其產品還包括計算器 、 微控制器以及多核處理器 。 德州儀器居世界半導體公司20強 。 德州儀器於1951年建立 。 它由地球物理業務公司 （ Geophysical Service Incorporated, GSI ） 整組而產生 。 這家公司最初生產地震工業和國防電子的相關設備 。 TI於1950年代初開始研究晶體管 ， 同時也製造了世界上第一個商用硅晶體管 。 1954年 ， TI研發製造了第一臺晶體管收音機 ， 1958年 ， 在TI中新研究實驗室工作的Jack Kilby傑克 · 基爾比發明了集成電路 。 1961年 ， TI爲美國空軍制造了第一臺集成電路電腦 。 50年代末期 ， TI開始研究紅外線技術 ， 隨後TI涉足製造導彈和炸彈的雷達系統 ， 導航和控制系統 。 世界上第一臺便攜式計算器由TI於1967年發明 。 20世紀70 、 80年代公司業務集中於家用電子產品 ， 如數字鐘錶 、 電子手錶 、 便攜式計算器 、 家用電腦以及各種傳感器 。 1997年公司將其國防業務出售給了雷神公司 。 2007年 ， 德州儀器被認爲是世界上最大的道德企業之一 。 2011年收購美國國家半導體 （ National Semiconductor ） 之後 ， TI擁有由約45000種模擬電路產品及客戶設計工具組成的投資組合 ， 這使TI成爲世界上最大模擬電路元器件生產廠商 。 2011年TI在財富500強中位列第175名 。 TI旗下業務有兩個主要分支 ： 半導體 （ SC ） 和教育技術 （ ET ） ， 其中半導體業務創造了公司收益的約96 % 。\n",
            "\n",
            "目前正在檢索的文章是:德州儀器\n",
            "\n",
            "現在要完成的是事實查核任務：從文章中找出與陳述句相關的句子，也有可能文章中不存在相關的句子。如果文章中的句子沒有陳述句的完整信息，請依照缺失的信息根據句子中的\"關鍵詞\"決定下一步需要檢索的文章(文章名稱必須是維基百科頁面，名詞)在思考下一步需要檢索的文章請記得不要是當前檢索的文章(當前檢索的文章名稱是什麼？)。如果思考證據句缺失的信息無法經由檢索其他文章中的句子完成任務則標籤為\"資訊不足\"\n",
            "\n",
            "先完成以上步驟給出思考過程。最後整理答案按照以下格式根據指示輸出：\n",
            "evidence:輸出所有證據句，包含已檢索的證據句以及從文章中檢索得到的證據句。並且不要使用: 數字 以及 · 以及 \n",
            " 來區分每一句話。\n",
            "doc:下一步需要檢索的文章。如果不再進行檢索其他文章則為\"None\"。如果缺失的信息無法經由檢索其他文章中的句子完成任務則標籤為\"Not enough info\"。\n",
            "state:如果需要繼續檢索其他文章則狀態為\"continue\"，如果已經找齊所有證據句則狀態為\"finish\"。\n",
            "\n",
            "System: You are ChatGPT,a large language model trained by OpenAI.\n",
            "Knowledge cutoff:2021-09\n",
            "Current data:[current date]\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[91m\u001b[1m Based on the given information, here is a breakdown of the process:\n",
            "\n",
            "陳述句: 德州儀器於1950年代初開始研究一種類似於閥門的固態半導體元件，製造了世界上第一個商用硅晶體管。\n",
            "\n",
            "根據陳述句，需要找到德州儀器於1950年代初研究固態半導體元件並製造第一個商用硅晶體管的相關信息。\n",
            "\n",
            "進行檢索的證據句是：德州儀器於1951年建立。它由地球物理業務公司（Geophysical Service Incorporated, GSI）整組而產生。這家公司最初生產地震工業和國防電子的相關設備。TI於1950年代初開始研究晶體管，同時也製造了世界上第一個商用硅晶體管。\n",
            "\n",
            "根據來自德州儀器條目的證據，得知德州儀器在1950年代初研究晶體管並製造了第一個商用硅晶體管。\n",
            "\n",
            "因此，根據提供的信息，我們已經找到了與陳述句相關的證據。對於下一步的檢索，我們可以結束此任務，因為已經找到了所有證據。\n",
            "\n",
            "結論：\n",
            "\n",
            "evidence: 德州儀器於1951年建立。它由地球物理業務公司（Geophysical Service Incorporated, GSI）整組而產生。這家公司最初生產地震工業和國防電子的相關設備。TI於1950年代初開始研究晶體管，同時也製造了世界上第一個商用硅晶體管。\n",
            "doc: None\n",
            "state: finish \u001b[0m\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new MyChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: \n",
            "證據句可能支持或反駁陳述句，以下提供的證據句與陳述句存在微小的差異，而只要有一點意思的的差異就會導致證據句\"反駁\"陳述句。按照以下步驟決定證據句支持或反駁陳述句:\n",
            "\n",
            "接下來請先找出證據句與陳述句之間的決定句子意思的關鍵字，接著仔細思考兩者是否存在差異：\"含義明顯衝突\"、\"無法經由推論得出結論\"、\"蘊含關係不明確\"。若有上述情況發生則視為反駁。\n",
            "\n",
            "請仔細檢查以上的狀況是否有發生，如果不存在則視為支持\n",
            "\n",
            "證據句:德州儀器於1951年建立。它由地球物理業務公司（Geophysical Service Incorporated, GSI）整組而產生。這家公司最初生產地震工業和國防電子的相關設備。TI於1950年代初開始研究晶體管，同時也製造了世界上第一個商用硅晶體管。\n",
            "陳述句:德州儀器於1950年代初開始研究一種類似於閥門的固態半導體元件，製造了世界上第一個商用硅晶體管。\n",
            "\n",
            "按照以上步驟決定證據句是支持還是反駁陳述句\n",
            "\n",
            "System: You are ChatGPT,a large language model trained by OpenAI.\n",
            "Knowledge cutoff:2021-09\n",
            "Current data:[current date]\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[96m\u001b[1m 根据所提供的证据句和陈述句进行比较，并找出它们之间的关键差异，让我们来检查是否存在\"含义明显冲突\"、\"无法经由推论得出结论\"或\"蕴含关系不明确\"等情况。\n",
            "\n",
            "陈述句中提到德州仪器在1950年代初开始研究一种类似于阀门的固态半导体元件，并且制造了世界上第一个商用硅晶体管。然而，在提供的证据句中并未明确提及这种元件是类似于阀门的。因此，我们可以注意到存在一个含义上的差异，这可能导致证据句与陈述句之间的冲突。\n",
            "\n",
            "根据这个分析，证据句\"反驳\"了陈述句。 \u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"***************************************************\")\n",
        "claim=claim=input(\"請輸入一個陳述句:\")\n",
        "\n",
        "text=\"KeyError\"\n",
        "evidence=\"需要從以下文章中檢索\"\n",
        "\n",
        "while(text==\"KeyError\"):\n",
        "    keywords=doc_chain.run(claim)\n",
        "    keyword=keywordParser(keywords)\n",
        "    text=get_wiki_text(wiki,keyword)\n",
        "\n",
        "while True:\n",
        "    label,doc,state=refreshState()\n",
        "    response=agent.run(claim=claim,evidence=evidence,keyword=keyword,text=text)\n",
        "    print(\"\\033[91m\\033[1m\" ,response, \"\\033[0m\\033[0m\")\n",
        "    evidence,doc,state=getAction(response)\n",
        "    #print(\"evidence:\",evidence,\"\\ndoc:\",doc)\n",
        "\n",
        "    IS_FINISH=(state==\"finish\" and doc==\"None\")\n",
        "    NOT_ENOUGH_INFO=(doc==\"Not enough info\" or (doc==keyword and state==\"finish\"))\n",
        "    IS_CONTINUE=(state==\"continue\")\n",
        "\n",
        "    if IS_FINISH:\n",
        "        answer=claimRetrieval.run(claim=claim,evidence=evidence)\n",
        "        print(\"\\033[96m\\033[1m\" ,answer, \"\\033[0m\\033[0m\")\n",
        "        break\n",
        "\n",
        "    elif NOT_ENOUGH_INFO:\n",
        "        print(\"\\033[90m\\033[1m\" + \"claim:\",claim,\"is NOT ENOUGH INFO\" \"\\033[0m\\033[0m\")\n",
        "        break\n",
        "\n",
        "    elif IS_CONTINUE:\n",
        "        if get_wiki_text(wiki,doc)!=\"KeyError\":\n",
        "            text=wiki[doc]\n",
        "        #evidence=evidence\n",
        "        continue\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
